{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9c104c0-980e-4d6a-9cb5-19ab8eba943b",
   "metadata": {},
   "source": [
    "##Introducción¶\n",
    "Una red neuronal artificial (RNA) preentrenada es un modelo que ha sido entrenado previamente y luego distribuído (incluyendo arquitectura y valores de pesos) para su uso posterior. De esta manera no es necesario que volver a entrenarlo, o bien, se puede utilizar parte de el ya entrenado, y entrenar otras partes, realizando un ajuste fino o fine tuning. Constituye una manera de reducir los recursos y tiempos necesarios para generar un sistema de aprendizaje automático.\n",
    "\n",
    "Hugging Face es una plataforma de aprendizaje automático, conocida por su librería de [Transformers](https://es.wikipedia.org/wiki/Transformador(modelo_de_aprendizajeautom%C3%A1tico)) creada para aplicaciones de procesamiento de lenguaje natural y que permite a los usuarios compartir conjuntos de datos y modelos de aprendizaje automático preentrenados. Su Model Hub contiene miles de modelos previamente entrenados de código abierto que cualquiera puede descargar y usar. Además dispone de varios cursos en los que se enseñan conceptos de aprendizaje automático a la vez que como utilizar sus herramientas.\n",
    "\n",
    "Una de las formas más básicas de usar para inferencia la librería Hugging Face Transformers es la función pipeline(), que permite conectar uno de sus modelos de con los pasos necesarios para su preprocesamiento y posprocesamiento, abstrayendo la mayor parte del código complejo de la librería y ofreciendo una API simple dedicada a varias tareas, entre ellas, audio, visión artificial, procesamiento de lenguaje natural y tareas multimodales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aec322-7373-43ef-aa7e-ec29986c4b86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "# Avoid some warning\n",
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf3b589-6127-4d4a-a512-3db66c226ffd",
   "metadata": {},
   "source": [
    "##Análisis de sentimientos¶\n",
    "El análisis de sentimientos premite clasificar textos de acuerdo a distintas etiquetas predeterminadas, como podrían ser textos positivos frente a negativos. Existen disintos modelos preentrenados disponibles para ello y para distintos idiomas, por ejemplo Spanish Sentiment Analysis Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042f8df5-49eb-4f21-a49f-7fedc97d27af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate a piepeline for spanish sentiment-analysis\n",
    "classifier = pipeline(model='VerificadoProfesional/SaBERT-Spanish-Sentiment-Analysis')\n",
    "text_1 = \"Nos alegra introducirles las redes neuronales artificiales preentrenadas a ustedes.\"\n",
    "classifier(text_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b759b1-b264-4f3b-b242-1d8650036a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_2 = \"Sería una lástima que el tema no les resultara interesante.\"\n",
    "classifier(text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b1ded3-0c3b-4c8c-9b45-22bd2a36c204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the classifier model\n",
    "print(classifier.model)\n",
    "# classifier.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0e19b0-19ae-4c38-bedf-5ae1f30a2002",
   "metadata": {},
   "source": [
    "... o también podemos realizar puntuaciones sobre los textos usando nlptown/bert-base-multilingual-uncased-sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac358c18-ff73-49bf-852d-1a19ae7b324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking texts\n",
    "classifier = pipeline(model='nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "print(classifier(text_1))\n",
    "print(classifier(text_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f478f4a2-65ac-4f57-9d72-56e79b0d797a",
   "metadata": {},
   "source": [
    "Podemos traducir los textos a inglés mediante Helsinki-NLP/opus-mt-es-en y realizar las puntuaciones nuevamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e81a65-997b-4c0f-8665-4d6f88bf3a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate the texts\n",
    "translator = pipeline(model='Helsinki-NLP/opus-mt-es-en')\n",
    "text_1_translated = translator(text_1)[0]['translation_text']\n",
    "text_2_translated = translator(text_2)[0]['translation_text']\n",
    "\n",
    "# And rank again\n",
    "print(text_1_translated, classifier(text_1_translated))\n",
    "print(text_2_translated, classifier(text_2_translated))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05221ab4-fac5-488c-bb80-186958409db4",
   "metadata": {},
   "source": [
    "##Detección de objetos en imágenes¶\n",
    "Detectar objetos en imágenes es una tarea clásica en visión por computador, y existen distintos modelos para ello, como DETR (End-to-End Object Detection) model with ResNet-50 backbone , estos modelos son más complejos que los utilizados anteriormente. Aquí lo probamos bajo una imagen extraída de Pixabay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf8e680-1381-4a4d-adcb-c193284b262b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some aditional library imports\n",
    "import requests\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Download an image\n",
    "url = \"https://cdn.pixabay.com/photo/2021/12/23/05/27/still-life-6888656_960_720.jpg\"\n",
    "image_data = requests.get(url, stream=True).raw\n",
    "image_sl = Image.open(image_data)\n",
    "\n",
    "# Convert PIL Image, convert it to a format suitable for matplotlib\n",
    "image_np = np.array(image_sl)\n",
    "\n",
    "# Create a matplotlib figure and axis\n",
    "fig, ax = plt.subplots(figsize=(12, 9))  # Width, height in inches\n",
    "ax.imshow(image_np)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfac344b-6c33-43d8-8634-dc828f5c71fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate and execute a pipeline for object detection\n",
    "object_detector = pipeline(model='facebook/detr-resnet-50')\n",
    "predictions = object_detector(image_sl)\n",
    "\n",
    "print(\"Number of detected objects:\", len(predictions))\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963d8e49-3df8-41a3-a998-9216d6c17aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matplotlib figure and axis\n",
    "# from https://wellsr.com/python/zero-shot-object-detection-with-hugging-face-transformers/\n",
    "fig, ax = plt.subplots(figsize=(12, 9))  # Width, height in inches\n",
    "ax.imshow(image_np)\n",
    "\n",
    "# Create a rectangle for each object\n",
    "for prediction in predictions:\n",
    "    box = prediction['box']\n",
    "    label = prediction['label']\n",
    "    score = prediction['score']\n",
    "    xmin, ymin, xmax, ymax = box.values()\n",
    "    rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, linewidth=1, edgecolor='red', facecolor='none')\n",
    "\n",
    "    # Add the rectangle to the axes\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "    # Add text\n",
    "    plt.text(xmin, ymin, f\"{label}: {round(score,2)}\", color='yellow', fontsize=10, verticalalignment='top')\n",
    "\n",
    "# Display the image\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e2ed0e-5a8b-4bb8-8df5-f8f2a91aa6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the object detector model\n",
    "print(object_detector.model)\n",
    "#object_detector.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b94b48-d817-44f8-9b81-2ab3fc35adfe",
   "metadata": {},
   "source": [
    "##Imagen a texto¶\n",
    "Generar texo a partir de una imagen otra de las tareas de visión por computador, puede suponer el detectar el texto dentro de una imagen (OCR de Optical Character Recognitio), con el modelo microsoft/trocr-base-handwritten probamos a reconocer texto escrito a mano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cdc2b1-0865-4dc8-8d4a-1878484388c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download an image\n",
    "#url = \"https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg\"\n",
    "url = \"https://bass-tian.com/es/img/Meticulosidad.jpg\"\n",
    "image_data = requests.get(url, stream=True).raw\n",
    "image = Image.open(image_data)\n",
    "\n",
    "#Convert PIL Image, convert it to a format suitable for matplotlib\n",
    "image_np = np.array(image)\n",
    "\n",
    "#Create a matplotlib figure and axis\n",
    "fig, ax = plt.subplots(figsize=(12, 9))  # Width, height in inches\n",
    "ax.imshow(image_np)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dd580f-3edf-4a02-b4a2-dc014001e2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate and execute a pipeline for ocr\n",
    "ocr = pipeline(model='microsoft/trocr-base-handwritten')\n",
    "ocr(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715ee682-d628-463b-a090-013f20364a2a",
   "metadata": {},
   "source": [
    "También se puede generar una leyenda que describa a una imagen, con el modelo nlpconnect/vit-gpt2-image-captioning probamos a reconocer el bodegón anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47aebc53-567e-4442-ad1c-6954ba058187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate and execute a pipeline for image caption\n",
    "captioner = pipeline(model='nlpconnect/vit-gpt2-image-captioning')\n",
    "captioner(image_sl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba87fdd-1fce-4311-8010-57a6e8fcdf3a",
   "metadata": {},
   "source": [
    "##Generación de imágenes¶\n",
    "Generar imágenes a partir de descripciones en lenguaje natural o prompts es una de las tareas de IA generativas más relevantes. Stable Diffusion es un modelo de código abierto para generar imágenes digitales de alta calidad a partir de descripciones en lenguaje natural. Basándonos en la implementación de Stable Diffusion en KerasCV se puede hacer llamadas a dicho modelo, aunque el tiempo de generación es bastante largo ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7697a9e9-dcd2-4d60-a02c-f221405c2f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "import keras_cv\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbc593d-2025-4981-9391-f10a046c404f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras_cv.models.StableDiffusion(img_width=512, img_height=512)\n",
    "\n",
    "prompt = \"photograph of a cowboy riding a bike\"\n",
    "images = model.text_to_image(prompt, batch_size=2)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i in range(len(images)):\n",
    "    ax = plt.subplot(1, len(images), i + 1)\n",
    "    plt.imshow(images[i])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8542d7-0ed6-4169-9768-3b6d060ab979",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
